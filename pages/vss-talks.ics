BEGIN:VCALENDAR
VERSION:2.0
PRODID:-//alvg calendar generator//PL related talks//EN
REFRESH-INTERVAL;VALUE=DURATION:PT4H
X-WR-CALNAME:VSS Talks
BEGIN:VEVENT
SUMMARY:VSS - Specification-Guided Reinforcement Learning
DTSTART;TZID=Asia/Calcutta:20230620T190000
DTEND;TZID=Asia/Calcutta:20230620T203000
UID:2023-06@vss-iarcs
DESCRIPTION:Meeting Details: Zoom link\, ID: 891 6409 4870\, Passcode: 082
 194\n\n Talk-Webpage: https://fmindia.cmi.ac.in/vss/\n\n\n\nhttps://us02we
 b.zoom.us/j/89164094870?pwd=eUFNRWp0bHYxRVpwVVNoVUdHU0djQT09\n\nTitle: Spe
 cification-Guided Reinforcement Learning\n\nSpeaker: Kishor Jothimurugan\n
 \nAbstract: Recent advances in Reinforcement Learning (RL) have enabled da
 ta-driven controller design for autonomous systems such as robotic arms an
 d self-driving cars. Applying RL to such a system typically involves encod
 ing the objective using a reward function (mapping transitions of the syst
 em to real values) and then training a neural network controller (from sim
 ulations of the system) to maximize the expected reward. However\, many ch
 allenges arise when we try to train controllers to perform complex long-ho
 rizon tasks---e.g.\, navigating a car along a complex track with multiple 
 turns. Firstly\, it is quite challenging to manually define well-shaped re
 ward functions for such tasks. It is much more natural to use a high-level
  specification language such as Linear Temporal Logic (LTL) to specify the
 se tasks. Secondly\, existing algorithms for learning controllers from log
 ical specifications do not scale well to complex tasks due to a number of 
 reasons including the use of sparse rewards and lack of compositionality. 
 Furthermore\, existing algorithms for verifying neural network policies (t
 rained using RL) cannot be easily applied to verify policies for complex l
 ong-horizon tasks due to large approximation errors. In this talk\, I will
  present my work on using logical specifications to specify RL tasks. Firs
 t\, I'll talk about algorithms for learning control policies from such spe
 cifications. Then\, I'll show how we can use logical task decompositions t
 o scale verification to long-horizons.
LOCATION:https://us02web.zoom.us/j/89164094870?pwd=eUFNRWp0bHYxRVpwVVNoVUd
 HU0djQT09
ORGANIZER:MAILTO:vss
PRIORITY:5
END:VEVENT
END:VCALENDAR
